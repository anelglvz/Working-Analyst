{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anelglvz/Working-Analyst/blob/main/ML-AI-for-the-Working-Analyst/Semana9_1_Working_Analyst_Explor_Explot_MultiArmedBandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bandido multibrazo\n"
      ],
      "metadata": {
        "id": "xVLSt5glSKFx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ADAbH73rfS"
      },
      "source": [
        "##Bibliotecas y Funciones\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ0Ts9FIHEmu"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maquinas = 4\n",
        "medias = np.zeros(maquinas)\n",
        "stds = np.zeros(maquinas)"
      ],
      "metadata": {
        "id": "ddCHUoH3WihM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTvgN0tvHi3A"
      },
      "source": [
        "def entorno_multi_armed_bandit(maquinas):\n",
        "    '''\n",
        "    Creamos el entorno para el problema \"multi_armed_bandit\" generando aleatoriamente\n",
        "    la distribución de probabilidad de los premios que otorga cada máquina\n",
        "    '''\n",
        "    medias1 = np.random.uniform(-5, 5, size=maquinas)\n",
        "    stds1 = np.random.uniform(0, 5, size=maquinas)\n",
        "    return medias1, stds1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srAAbKjhKM_E"
      },
      "source": [
        "def init_Q(maquinas):\n",
        "  '''Inicializa el vector Q en ceros, el vector Q representa el valor esperado\n",
        "   de recompensa de cada máquina'''\n",
        "  Q =np.zeros(maquinas)\n",
        "  return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2yLnkZKLUlp"
      },
      "source": [
        "def selecciona_maquina(maquinas):\n",
        "    '''selecciona una máquina aleatoriamente con distribución unifome'''\n",
        "    selec = np.random.choice(range(maquinas))\n",
        "    return selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoF4BrM1gfA-"
      },
      "source": [
        "def selecciona_maquina_expl(Q):\n",
        "    '''selecciona la maquina con el maximo valor de la ganancia esperada'''\n",
        "    selec = np.argmax(Q)\n",
        "    return selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9atc3BHgU2d"
      },
      "source": [
        "def selecciona_maquina_egd(maquinas, epsilon):\n",
        "    '''selecciona una con la estrategia epsilon decreasing greedy'''\n",
        "    p = np.random.uniform(0,1)\n",
        "\n",
        "    # cuándo epsilon es pequeño, se escoge la maquina con mayor ganancia\n",
        "    if p < (1 - epsilon):\n",
        "        selec = np.argmax(Q)\n",
        "        return selec  \n",
        "\n",
        "    # caso contrario, se escoge una al azar\n",
        "    else: \n",
        "        selec = np.random.choice(range(maquinas))\n",
        "        return selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUv7oB7VId1t"
      },
      "source": [
        "def calcula_recompensa(selec):\n",
        "  '''calcula la recompensa de jugar en una determinada máquina'''\n",
        "  r = int(np.random.normal(medias[selec], stds[selec], 1))\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfwiInqOkBX"
      },
      "source": [
        "def actualiza_Q (Q, selec, r, veces_maq):\n",
        "    '''actualiza el valor esperados de recompensa de la máquina seleccionada'''\n",
        "    Q[selec] = Q[selec] + 1/(veces_maq[selec])*(r - Q[selec])\n",
        "    return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo anterior es: $Q_{k+1}(a)=\\frac{1}{k+1} (r_1+\\ldots+r_{k+1})$\n",
        "para una maquina $a$, que es lo mismo a: \n",
        "$$ Q_{k+1}(a) = Q_k(a) +\\frac{1}{k+1} (r_{k+1}-Q_k(a))  $$"
      ],
      "metadata": {
        "id": "mOnODcBUTHXk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzVeNtKjpHrT"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKA6ZoIgr9wq"
      },
      "source": [
        "## Definimos nuestro entorno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FJ8WPRopBiH"
      },
      "source": [
        "### Creamos el entorno definiendo el comportamiento de cada máquina\n",
        "\n",
        "premio_medio  = np.array([5,  1,  0, -10])\n",
        "desv_estandar = np.array([1, 0.1, 5,  1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos como se comportarian las máquinas tomando muestreos con dichas mediasy  desviaciones estándar\n",
        "for maquina in range(4):\n",
        "\n",
        "  s = np.random.normal(premio_medio[maquina], desv_estandar[maquina],10000)\n",
        "  plt.xlim(-12,8)\n",
        "  plt.ylim(0,5)\n",
        "  plt.hist( s, bins=30,  density=True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JEnrsD3ZyZI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Que hace un jugador en el casino?"
      ],
      "metadata": {
        "id": "b-FpFTSl1UF0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXUF3Prrubk"
      },
      "source": [
        "* inicializamos en cero la variable que guardará los premios o perdidas acumuladas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Zlrmeum_3q"
      },
      "source": [
        "ganado = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMOjxNywtLLw"
      },
      "source": [
        "* seleccionamos una de las maquinas, se juega en ella y se obtiene la recompensa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGjez-pgtLLz"
      },
      "source": [
        "selec = 2\n",
        "r = int(np.random.normal(premio_medio[selec], desv_estandar[selec], 1))\n",
        "ganado += r\n",
        "print('recompensa(r):', r)\n",
        "print('ganado:', ganado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fOSdzrXvkOE"
      },
      "source": [
        "##  ¿Cómo guardamos la información de los premios obtenidos de cada máquina?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnPBUn3qnYn2"
      },
      "source": [
        "# Inicializa el vector Q en ceros, donde guardaremos la información\n",
        "n_maquinas = 4 \n",
        "Q = init_Q(n_maquinas)\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCwiS-nR0_7D"
      },
      "source": [
        "# Vamos a jugar cierto número de veces (episodios)\n",
        "episodios = 15\n",
        "n_maquinas = 4\n",
        "ganado = 0\n",
        "\n",
        "veces_maq = np.zeros(n_maquinas)      # guardaremos las veces que se ha jugado por máquina\n",
        "for ep in range(1, episodios+1):\n",
        "  selec = np.random.choice([0,1,2,3]) # selección aleatoria -> exploración\n",
        "  veces_maq[selec] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "  r = int(np.random.normal(premio_medio[selec], desv_estandar[selec], 1))\n",
        "  Q[selec] = Q[selec] + 1/(veces_maq[selec])*(r - Q[selec])\n",
        "  ganado += r\n",
        "  print('Episodio_{}, Máquina {}, Premio = {}, Premio_acum = {}, Veces_maq:{} ,Q:{}   \\n'.format(ep,\n",
        "                                                                   selec,\n",
        "                                                                   r, \n",
        "                                                                   ganado,\n",
        "                                                                   veces_maq,Q.round(2)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVXPXUUw38V5"
      },
      "source": [
        "¿Sería correcto si, en lugar de escojer la máquina de forma aleatoria, escojemos la que más rendimientos ha dado?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "id": "hzGRQEpALn7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulLG8yCP0xLy"
      },
      "source": [
        "selec = np.argmax(Q)  # selección usando la estrategia de explotación\n",
        "selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX6_4S3c8JWN"
      },
      "source": [
        "¿Podemos pensar en una estrategia combinada?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R-1PKtw8lMb"
      },
      "source": [
        "p = np.random.uniform(0, 1 )  # escojemos un valor entre 0 y 1\n",
        "ε = .5 # valor que disminuye desde 1 hasta 0 según avance el aprendizaje\n",
        "print(f'p = {p}, ε = {ε}')\n",
        "print('¿p es menor que 1 - ε?', p < (1 - ε))\n",
        "if p < (1 - ε):\n",
        "    # Explotación\n",
        "    print('explotación')\n",
        "    selec = np.argmax(Q)\n",
        "else: \n",
        "    # Exploración\n",
        "    print('exploración')\n",
        "    selec = np.random.choice(range(n_maquinas))\n",
        "selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmve4Vj7CJAf"
      },
      "source": [
        "Vamos a calcular  ε en función de los episodios, de manera decreciente\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V2gOQ1xCHLx"
      },
      "source": [
        "episodios = 100\n",
        "ε = np.exp(-5 * np.linspace(0, 1, episodios))\n",
        "ε"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuxbAT2X-ian"
      },
      "source": [
        "fig =plt.figure()\n",
        "plt.plot(ε)\n",
        "plt.xlabel('episodio', fontsize=16)\n",
        "plt.ylabel('ε', fontsize=16)\n",
        "fig.suptitle('ε decreciente con el numero de episodios', fontsize=18)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHGYfGfVtZhW"
      },
      "source": [
        "# Aprendizaje con distintas estrategias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QF9KYgctZ9P"
      },
      "source": [
        "### Definimos el número de máquinas o brazos del problema\n",
        "n_maquinas = 4\n",
        "\n",
        "### Creamos el entorno\n",
        "np.random.seed(4)\n",
        "medias, stds = entorno_multi_armed_bandit(n_maquinas)  # inicializa la distribución de probabilidad de cada máquina"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-3477k0izn_"
      },
      "source": [
        "### Definimos el número de episodios (juegos)\n",
        "episodios = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medias"
      ],
      "metadata": {
        "id": "G25N0TSnNat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stds"
      ],
      "metadata": {
        "id": "lQWX4_8ENcAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBOk8P63f7Hn"
      },
      "source": [
        "## Exploración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGdvub9qSwmJ"
      },
      "source": [
        "### Inicializamos el vector Q  \n",
        "Q = init_Q(n_maquinas)  \n",
        "ganado = 0\n",
        "veces_maq = np.zeros(n_maquinas)      # guardaremos las veces que se ha jugado por máquina\n",
        "\n",
        "for i in range(n_maquinas):  # creamos un ciclo para jugar en cada máquina una vez\n",
        "    veces_maq[i] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "    r = calcula_recompensa(i)\n",
        "    Q = actualiza_Q(Q, i, r, veces_maq)\n",
        "    ganado += r\n",
        "    print(Q)\n",
        "\n",
        "print(veces_maq)\n",
        "\n",
        "for episodio in range(n_maquinas+1,episodios+1):\n",
        "    selec = selecciona_maquina(n_maquinas) # elegimos al azar\n",
        "    veces_maq[selec] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "    # Calcula el premio de esa acción\n",
        "    r = calcula_recompensa(selec)\n",
        "    # Actualiza la información de lo ganado por cada máquina\n",
        "    Q = actualiza_Q(Q, selec, r, veces_maq)\n",
        "  \n",
        "    ganado += r\n",
        "  \n",
        "print(\"La ganancia total es de: \", ganado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veces_maq"
      ],
      "metadata": {
        "id": "z5Q-bqGBLMm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDUtGQYnZCL"
      },
      "source": [
        "## Explotación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMTfsHDUnfH4"
      },
      "source": [
        "### Inicializamos el vector Q  \n",
        "Q = init_Q(n_maquinas)  \n",
        "ganado = 0\n",
        "veces_maq = np.zeros(n_maquinas)      # guardaremos las veces que se ha jugado por máquina\n",
        "\n",
        "for i in range(n_maquinas):  # creamos un ciclo para jugar en cada máquina una vez\n",
        "    veces_maq[i] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "    r = calcula_recompensa(i)\n",
        "    Q = actualiza_Q(Q, i, r, veces_maq)\n",
        "    ganado += r\n",
        "\n",
        "print(Q)\n",
        "\n",
        "for episodio in range(n_maquinas+1 ,episodios+1): # el resto de los episodios los jugamos con una estrategia codiciosa\n",
        "    selec = selecciona_maquina_expl(Q)\n",
        "    veces_maq[selec] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "    r = calcula_recompensa(selec)\n",
        "    Q = actualiza_Q(Q, selec, r, veces_maq)\n",
        "    ganado += r\n",
        "print(\"La ganancia total es de: \", ganado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veces_maq"
      ],
      "metadata": {
        "id": "NiNOwlSAN282"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXk8RpAf_S5"
      },
      "source": [
        "## Epsilon decreasing greedy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comando para ver mejor números sin notación científica\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "epsilon = 0.1"
      ],
      "metadata": {
        "id": "6orTHV3v6XUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwluzms4f_S7"
      },
      "source": [
        "### Inicializamos el vector Q  \n",
        "Q = init_Q(n_maquinas)  \n",
        "epsilon = np.exp(-5 * np.linspace(0, 1, episodios))  # creamos el vector con un epsilon para cada episodio (No afecta al ser modificado eb linea 16)\n",
        "ganado = 0\n",
        "veces_maq = np.zeros(n_maquinas)      # guardaremos las veces que se ha jugado por máquina\n",
        "\n",
        "for i in range(n_maquinas):  # creamos un ciclo para jugar en cada máquina una vez\n",
        "    veces_maq[i] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "    r = calcula_recompensa(i)\n",
        "    Q = actualiza_Q(Q, i, r, veces_maq)\n",
        "    ganado += r\n",
        "\n",
        "print(veces_maq)\n",
        "\n",
        "for episodio in range(n_maquinas+1 ,episodios+1):\n",
        "    #eps = epsilon[episodio-1]  #epsilon decreciente\n",
        "    eps=0.1\n",
        "    selec = selecciona_maquina_egd(n_maquinas, eps)\n",
        "\n",
        "    veces_maq[selec] += 1               # actualizamos las veces que se ha jugado x máquina\n",
        "    r = calcula_recompensa(selec)\n",
        "    Q = actualiza_Q(Q, selec, r, veces_maq)\n",
        "    ganado += r\n",
        "\n",
        "print(\"La ganancia total es de: \", ganado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veces_maq"
      ],
      "metadata": {
        "id": "TRmYWVCS7U_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9EM0mu3mB99"
      },
      "source": [
        "# Referencias:\n",
        "\n",
        "[1]A. Aristizabal, «Understanding Reinforcement Learning Hands-On: Multi-Armed Bandits», Medium, oct. 19, 2020. https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc (accedido jul. 30, 2021).\n",
        "\n"
      ]
    }
  ]
}
