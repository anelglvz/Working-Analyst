{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwFNmMrb6zetMWT/0AgZdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anelglvz/Working-Analyst/blob/main/Maximize_Revenues_with_Thompson_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thompson Sampling Algorithm"
      ],
      "metadata": {
        "id": "xnCmmkEHx_DA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp4lUqlJGfNB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import scipy.stats as ss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See this for more details: https://en.wikipedia.org/wiki/Conjugate_prior"
      ],
      "metadata": {
        "id": "DxOl8Wjqqol0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Thompson Sampling for binomial distributed Revenues with beta apriori\n",
        "\n"
      ],
      "metadata": {
        "id": "m-PH1afMw9m1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data and Strategies"
      ],
      "metadata": {
        "id": "d9CZEUuntLgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10000   # number of rounds - customer visiting web page with Ad\n",
        "d = 9       # number of strategies"
      ],
      "metadata": {
        "id": "g17C6fBBNUUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversion_rates = [0.05,0.13,0.09,0.16,0.11,0.04,0.20,0.08,0.01]"
      ],
      "metadata": {
        "id": "_woYp8OfNdlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the reward matrix (num of rounds x result for each strategy)\n",
        "np.random.seed(4)\n",
        "rewards = np.zeros((N,d))\n",
        "# populate matrix according to the conversion rates\n",
        "for i in range(N):\n",
        "    for j in range(d):\n",
        "        if np.random.rand() <= conversion_rates[j]:   # np.random.rand(d0, d1, ..., dn) returns Random values [0,1[ in a given shape d0xd1x...\n",
        "            rewards[i,j] = 1"
      ],
      "metadata": {
        "id": "SMoDRXUFNkNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rewards)\n",
        "print(rewards.shape)"
      ],
      "metadata": {
        "id": "1KNcY_BtNplS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beta Distribution"
      ],
      "metadata": {
        "id": "vzLZYWe7tD-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xaux = np.linspace(0,1,100)\n",
        "yaux = ss.beta.pdf(xaux,1,1)\n",
        "plt.plot(xaux,yaux)"
      ],
      "metadata": {
        "id": "2CFyH1hXnrGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement random strategy and Thompson Sampling for comparison"
      ],
      "metadata": {
        "id": "sKhHQny3NwN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_strategies_random = []\n",
        "selected_strategies_thompson = []\n",
        "total_rewards_random = 0\n",
        "total_rewards_thompson = 0\n",
        "numbers_of_rewards_1 = np.zeros(d)\n",
        "numbers_of_rewards_0 = np.zeros(d)\n",
        "reward_per_strategy = np.zeros(d)\n",
        "regret_curve_random = []\n",
        "regret_curve_thompson = []\n",
        "\n",
        "for n in range(N):\n",
        "    \n",
        "    # Random strategy\n",
        "    selected_strategy_random = random.randrange(d)\n",
        "    selected_strategies_random.append(selected_strategy_random)\n",
        "    total_rewards_random+= rewards[n,selected_strategy_random]\n",
        "    \n",
        "    # Thompson Sampling Strategy\n",
        "    max_random_beta = 0\n",
        "    startegy_with_max_beta = 0\n",
        "    random_beta = np.zeros(d)\n",
        "    \n",
        "    # for each strategy, random draw from beta distribution with 2 parameters then select highest\n",
        "    for i in range(d):\n",
        "        random_beta[i] = ss.beta.rvs(numbers_of_rewards_1[i]+1, numbers_of_rewards_0[i]+1) # return value in range 0,1\n",
        "        startegy_with_max_beta_draw = np.argmax(random_beta)\n",
        "        max_random_beta = random_beta[startegy_with_max_beta_draw]\n",
        "\n",
        "    # Update Beta distribution parameters of selected strategy\n",
        "    if rewards[n,startegy_with_max_beta_draw]==1:\n",
        "        numbers_of_rewards_1[startegy_with_max_beta_draw]+=1\n",
        "    else:\n",
        "        numbers_of_rewards_0[startegy_with_max_beta_draw]+=1\n",
        "\n",
        "  # Update Thompson sampling strategy KPIs\n",
        "    selected_strategies_thompson.append(startegy_with_max_beta_draw)\n",
        "    total_rewards_thompson += rewards[n,startegy_with_max_beta_draw]\n",
        "    \n",
        "    # Score per strategy\n",
        "    for i in range(d):\n",
        "        reward_per_strategy[i] +=rewards[n, i]\n",
        "    \n",
        "    # Regret as the difference between slected strategy and best strategy\n",
        "    regret = max(reward_per_strategy) - total_rewards_random\n",
        "    regret_curve_random.append(regret)\n",
        "    regret = max(reward_per_strategy) - total_rewards_thompson\n",
        "    regret_curve_thompson.append(regret)"
      ],
      "metadata": {
        "id": "O7A8CRczXXb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Measure performance\n"
      ],
      "metadata": {
        "id": "S5xMx88HOIA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the Absolute and Relative Return\n",
        "# in absolute monetary value assuming 1K€ extra revenue for each premium plan\n",
        "absolute_return = (total_rewards_thompson - total_rewards_random) \n",
        "# profit increase in % vs random strategy\n",
        "relative_return = (total_rewards_thompson - total_rewards_random) / total_rewards_random * 100"
      ],
      "metadata": {
        "id": "tGryUtP5N7Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Performance achieved over {N} samples and the assumed strategy conversion rate:')\n",
        "print('Absolute return: {:.0f}K€ extra profits'.format(absolute_return))\n",
        "print('Relative return: {:.0f} % profit increase'.format(relative_return))"
      ],
      "metadata": {
        "id": "mqhN4RE8OLre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot histogram of selected strategies"
      ],
      "metadata": {
        "id": "_ekZR7WHOR7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(selected_strategies_thompson, align='mid', rwidth=0.5)\n",
        "plt.title('Histogram of selected strategies')\n",
        "plt.xlabel('Strategy')\n",
        "plt.ylabel('Times strategy is selected')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYgg4na1OOoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(range(1000), selected_strategies_thompson[:1000], marker='.', alpha=0.5)"
      ],
      "metadata": {
        "id": "dWv9X70yOamX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_scatter(k):\n",
        "    plt.scatter(range(10*k,10*k+10), selected_strategies_thompson[10*k:10*k+10], marker='.', c='b', alpha=0.5)"
      ],
      "metadata": {
        "id": "xK8EniWqObz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a writer which uses ffmpeg and records at 20 fps with a bitrate of 1800\n",
        "import matplotlib.animation as animation\n",
        "Writer = animation.writers['ffmpeg']\n",
        "writer = Writer(fps=5, metadata=dict(artist='Me'), bitrate=1800)"
      ],
      "metadata": {
        "id": "g4hBlhTqOfE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,5))\n",
        "plt.xlabel('Steps',fontsize=10)\n",
        "plt.title('Convergence of Strategy selection by Thompson Sampling',fontsize=10)\n",
        "plt.ylabel('Selected strategy',fontsize=10)\n",
        "ani = matplotlib.animation.FuncAnimation(fig, plot_scatter, frames=100, repeat=True)\n",
        "#plt.show()\n",
        "ani.save('thompsonsampling.mp4', writer=writer)"
      ],
      "metadata": {
        "id": "bDX1G0uyOku2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Regret curves"
      ],
      "metadata": {
        "id": "vlFczmUZOsHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(regret_curve_random, label='Random strategy')\n",
        "plt.plot(regret_curve_thompson, label='Thompson Sampling')\n",
        "plt.title('Regret Curves')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('gap with best strategy at each step')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Avh1lhcEOpoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(N), regret_curve_thompson, color='orange')\n",
        "plt.title('Regret Curve Thompson Sampling strategy')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('gap with best strategy at each step')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mwMS0H2zOwBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Thompson Sampling for normally distributed Revenues with variance known with normal apriori"
      ],
      "metadata": {
        "id": "rBG-mISZ4iHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can consider a Revenues such that $X \\sim N(\\mu,σ^2)$ and normal distribution for apriori."
      ],
      "metadata": {
        "id": "9aCI8l3F_iTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data and Strategies"
      ],
      "metadata": {
        "id": "Ldvk60n5wxXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10000\n",
        "d = 6\n",
        "mu = [9.5,7.3,5.8,9.9,8.8,7.1]\n",
        "sigma = 4\n",
        "\n",
        "rewards = np.zeros([N,d])\n",
        "for j in range(N):\n",
        "  for i in range(d):\n",
        "    rewards[j,i] = ss.norm.rvs(mu[i], sigma)\n",
        "\n",
        "print(rewards)"
      ],
      "metadata": {
        "id": "nQq5xbzO_0o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_per_strategy = np.mean(rewards, axis = 0)\n",
        "print(reward_per_strategy)"
      ],
      "metadata": {
        "id": "aert_BfyDGon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement Thompson Sampling"
      ],
      "metadata": {
        "id": "NCQpsilVxXvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = np.zeros(d)\n",
        "mu0 = 5\n",
        "mus = [mu0]*d\n",
        "selected_strategies_thompson = []\n",
        "sigma0 = 1\n",
        "sigs = [sigma0]*d\n",
        "\n",
        "for n in range(N):\n",
        "  for i in range(d):\n",
        "    theta[i] = ss.norm.rvs(mus[i],sigs[i])\n",
        "  \n",
        "  # Update Nomarl distribution parameters of selected strategy\n",
        "    if(all(theta)>0):\n",
        "      startegy_with_max_norm_draw = np.argmax(theta)\n",
        "      rewardst = rewards[:n,startegy_with_max_norm_draw]\n",
        "      mus[startegy_with_max_norm_draw] = ((n+1)/sigma**2+1/sigma0**2)**(-1)*((n+1)/(sigma**2)*(np.mean(rewardst))+1/(sigma0**2)*mu0)\n",
        "      sigs[startegy_with_max_norm_draw] = ((n+1)/sigma**2+1/sigma0**2)**(-1/2)\n",
        "\n",
        "  # Update Thompson sampling strategy KPIs\n",
        "  selected_strategies_thompson.append(startegy_with_max_norm_draw)\n"
      ],
      "metadata": {
        "id": "AdshcLR3LR4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot histogram of selected strategies"
      ],
      "metadata": {
        "id": "7k2MMCQzxjny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(selected_strategies_thompson, align='mid', rwidth=0.5)\n",
        "plt.title('Histogram of selected strategies')\n",
        "plt.xlabel('Strategy')\n",
        "plt.ylabel('Times strategy is selected')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2gZ9GXavHm4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(range(1000), selected_strategies_thompson[:1000], marker='.', alpha=0.5)"
      ],
      "metadata": {
        "id": "uV3eS5JQfeE8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}