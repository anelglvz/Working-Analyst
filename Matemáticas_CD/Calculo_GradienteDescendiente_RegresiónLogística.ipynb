{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anelglvz/Working-Analyst/blob/main/Matem%C3%A1ticas_CD/Calculo_GradienteDescendiente_Regresi%C3%B3nLog%C3%ADstica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción\n",
        "\n",
        "Se verá como funciona el algoritmo de Gradiente Descendiente aplicado a una regresión logística que asignará las probabilidades utilizando la función softmax."
      ],
      "metadata": {
        "id": "4oifWcjIiD7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias"
      ],
      "metadata": {
        "id": "Nyex2mbhUAYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PBpjrBycDrRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solo para que al mostrarnos los arreglos no nos de los valores grandes en notación científica\n",
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "e6EHiIOTVg8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos e interpretación\n"
      ],
      "metadata": {
        "id": "oXC7fZc5UCdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "f4hILff7UzDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Curso-Algebra/semana10/pruebas_datos.csv')"
      ],
      "metadata": {
        "id": "iHs4VEOGT90s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FK5Uw7BZU8s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Los valores 1 son NON-HFT y los 0 son HFT\n",
        "df.type.value_counts()"
      ],
      "metadata": {
        "id": "iU7IEu3FVEjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregando descripción a los datos\n",
        "\n",
        "Los datos son públicos, fueron utilizados en una competencia que pueden consultar [AQUÍ](https://challengedata.ens.fr/participants/challenges/50/) (para poder ingresar necesitarán registrarse y luego copiar la dirección en su navegador). En esta competencia registraron muchas características de \"traders\", para tratar de clasificarlos en HFT (High Frequency Traders) o NON-HFT, esto porque es de gran interes tratar de estudiar el impacto que tienen los HFT en los mercados, ya que lo que hacen es realizar intercambios dando ordenes de intercambio a un mecanismo llamado Limit Order Book (LOB), al cual le ordenan realizar compras o ventas a precios que ellos deseen.\n",
        "\n",
        "## Descripción de las variables\n",
        "1. NbTradeVenueMic (23) : Número de centros de negociación en los que opera el jugador del mercado\n",
        "\n",
        "De todos los centros de negociación, estadísticas sobre el número de operaciones observadas por segundo\n",
        "\n",
        "2. MaxNbTradesBySecond (24)\n",
        "3. MeanNbTradesBySecond (25) \n",
        "\n",
        "Estadísticas sobre el delta de tiempo observado entre dos operaciones en la plataforma de negociación TV_1[1]:\n",
        "\n",
        "4. min_dt_TV1 (26)\n",
        "5. med_dt_TV1 (28)\n",
        "6. mean_dt_TV1 (27)\n",
        "\n",
        "Estadísticas sobre el delta de tiempo observado entre dos operaciones que se producen en la plataforma de negociación TV_1 y luego en la plataforma de negociación TV_2:\n",
        "\n",
        "7. min_dt_TV1_TV2 (29)\n",
        "8. med_dt_TV1_TV2 (31)\n",
        "9. mean_dt_TV1_TV2 (30)\n",
        "\n",
        "Estadísticas sobre el delta de tiempo observado entre dos operaciones que se producen en la plataforma de negociación TV_1 y luego en la plataforma de negociación TV_3:\n",
        "\n",
        "10. min_dt_TV1_TV3 (32)\n",
        "11. med_dt_TV1_TV3 (34)\n",
        "12. mean_dt_TV1_TV3 (33)\n",
        "\n",
        "Estadísticas sobre el delta de tiempo observado entre dos operaciones que se producen en la plataforma de negociación TV_1 y luego en la plataforma de negociación TV_4 :\n",
        "\n",
        "13. min_dt_TV1_TV4 (35)\n",
        "14. med_dt_TV1_TV4 (37)\n",
        "15. mean_dt_TV1_TV4(36)\n",
        "\n",
        "De todos los centros de negociación, número de segundos durante el día de negociación donde se observa al menos una operación del jugador del mercado i\n",
        "\n",
        "16. NbSecondWithAtLeatOneTrade(38)\n",
        "\n",
        "En la plataforma de negociación TV_1, tres proporciones entre el número de todo tipo de eventos[2] enviados a la LOB y:\n",
        "\n",
        "17. el número de operaciones (OTR) (4)\n",
        "18. el número de eventos de tipo cancelación (OCR) (5)\n",
        "19. el número de eventos de tipo de modificación (OMR) (6)\n",
        "\n",
        "En el centro de negociación TV_1, estadísticas durante el delta de tiempo observado entre dos eventos de todo tipo enviados:\n",
        "\n",
        "20. min_time_two_events (7)\n",
        "21. mean_time_two_events (8)\n",
        "22. 10_p_time_two_events (9)\n",
        "23. med_time_two_events (10)\n",
        "24. 25_p_time_two_events (11)\n",
        "25. 75_p_time_two_events (12)\n",
        "26. 90_p_time_two_events (13)\n",
        "27. max_time_two_events (14)\n",
        "\n",
        "En la plataforma de negociación TV_1, estadísticas a lo largo de la vida útil observada de las órdenes canceladas:\n",
        "\n",
        "28. min_lifetime_cancel (15)\n",
        "29. mean_lifetime_cancel (16)\n",
        "30. 10_p_lifetime_cancel (17)\n",
        "31. med_lifetime_cancel (18)\n",
        "32. 25_p_lifetime_cancel (19)\n",
        "33. 75_p_lifetime_cancel (20)\n",
        "34. 90_p_lifetime_cancel (21)\n",
        "35. max_lifetime_cancel (22)\n",
        "\n",
        "Extra variables\n",
        "\n",
        "36. Type: HF, Non HFT (39)\n",
        "37. Day\n",
        "38. Share\n",
        "39. Trader\n",
        "40. Index\n"
      ],
      "metadata": {
        "id": "SCtqgR4MZsTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "jU9HDQM8VTfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prob = np.array(df.drop(['type'], axis=1))\n",
        "X_prob"
      ],
      "metadata": {
        "id": "FIcEwUB_WMlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prob.shape"
      ],
      "metadata": {
        "id": "ysTvZku2ILgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prob = np.array(df['type'])\n",
        "Y_prob"
      ],
      "metadata": {
        "id": "UpZPlZlnWARR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prob.shape"
      ],
      "metadata": {
        "id": "GyU40YXb0dIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs = 25\n",
        "num_outputs = 2\n",
        "\n",
        "W = np.random.normal(0, 0.1, (num_inputs, num_outputs)) # Jugaremos con este parametro\n",
        "B = np.zeros(num_outputs) # Jugaremos con este parametro"
      ],
      "metadata": {
        "id": "HgYtgbebimh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W)\n",
        "print(B)"
      ],
      "metadata": {
        "id": "gDLgt_L3VZNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de la operación SoftMax\n",
        "\n",
        "Primero, notemos que dado un arreglo de numpy, nosotros podemos relizar sumas de diferentes formas utilizando el método \"sum()\". Al daro distintos argumentos se reliza de distintos modos.\n",
        "\n",
        "Si no pasamos ningún parámetro a \"sum()\", lo que obtendremos será la suma de todas las entradas de nuestra matriz (o arreglo o tensor). \n",
        "\n",
        "Si damos como parámetro el \"0\", lo que nos regresará será la suma por columnas (es decir, en el eje 0)\n",
        "\n",
        "Si damos como parámetro el \"1\", nos regresará la suma por filas (es decir, en el eje 1)"
      ],
      "metadata": {
        "id": "yLTx9XpmoPuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "X"
      ],
      "metadata": {
        "id": "c1tKKzybKRFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.sum(keepdims=True), X.sum(0, keepdims=True), X.sum(1, keepdims=True)"
      ],
      "metadata": {
        "id": "0prrE4B7ixOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto podemos implementar de modo muy sencillo la función SoftMax, que consiste de 3 pasos:\n",
        "\n",
        "1. Obtener el exponenete de cda uno de nuestros elementos.\n",
        "2. Tomamos la suma de los elementos de su fila\n",
        "3. Dividimos cada fila por su constante de normalización (sería una suma de exponenciales)\n",
        "\n",
        "La expresión matemática de la función SoftMax es la siguiente:\n",
        "\n",
        "\n",
        "$$ \\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}. $$"
      ],
      "metadata": {
        "id": "1qKW7Pn2C55P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(X):\n",
        "    X_exp = np.exp(X)\n",
        "    partition = X_exp.sum(1, keepdims=True)\n",
        "    return X_exp / partition"
      ],
      "metadata": {
        "id": "nPCYu0LHnqxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un ejemplo con la matriz de 2X3 creada anteriormente."
      ],
      "metadata": {
        "id": "qNDcmahzEwGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(X)"
      ],
      "metadata": {
        "id": "0fL_0uvtntuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si observan con cuidado, pueden ver que con esta operación estamos transformando todas nuestras entradas en números positivos. Mas aún, cada fila suma 1, es decir, la convertimos en una distribución de probabilidad."
      ],
      "metadata": {
        "id": "CqHlJW4-E71h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.normal(0, 1, (2, 5))\n",
        "x_prob = softmax(x)\n",
        "x_prob, x_prob.sum(1)"
      ],
      "metadata": {
        "id": "27w0BqwBpPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definiendo el modelo\n",
        "\n",
        "Aquí pueden ver que tipo de modelo vamos a implementar. En este caso es una regresión softmax."
      ],
      "metadata": {
        "id": "cyDAzg-TplBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net(X,W,b):\n",
        "    return softmax(np.dot(X.reshape((-1, W.shape[0])), W) + b)"
      ],
      "metadata": {
        "id": "Xt5hs-R5pmzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ¿Que nos dará como salida ésta operación?\n",
        "\n",
        "2. ¿Porqué?"
      ],
      "metadata": {
        "id": "y5Lg62VfGcnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definiendo la función de perdida"
      ],
      "metadata": {
        "id": "v-lGeJHLptUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la función Cross-Entropy, una de las mas populares dado que es buena para problemas de clasficación y en la actualidad, en machine learning son mas los problemas de clasificación que los de regresión."
      ],
      "metadata": {
        "id": "-aIVnMDipv8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función es la siguiente:\n",
        "$$ -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}), $$\n",
        "\n",
        "donde $M$ es la cantidad de clases que tenemos, $log$ la función de logaritmo natural, $y$ es $0$ o $1$ dependiendo de la etiqueta real de la clase $c$ y por último $p_{o,c}$ es la probabilidad predicha de que la observación $o$ pertenezca a la clase $c$."
      ],
      "metadata": {
        "id": "5W2-3hHaHLOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para calcular esto, en lugar de utilizar un ciclo \"for\" (que a veces termina siendo menos optimizado), utilizaremos funciones de numpy."
      ],
      "metadata": {
        "id": "p8VFVTHVHKCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "    return np.sum(- np.log(y_hat[range(len(y_hat)), y])) / y.shape[0]"
      ],
      "metadata": {
        "id": "pHEEjhW3qqgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# El resultado final es lo que alimentará a la cross-entropy, es decir etiqueta \n",
        "# de que pertenece a la clase 1 por la probabilidad de pertenecer a la clase 1\n",
        "y = np.array([0, 2])\n",
        "y_hat = np.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y_hat[[0, 1], y]"
      ],
      "metadata": {
        "id": "VSkWQRm1p1h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_each(y_hat, y)"
      ],
      "metadata": {
        "id": "-C9idG4Lsk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precisión de la clasificación"
      ],
      "metadata": {
        "id": "y0C0Wio7s5Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En un problema de clasificación multi-clase, es decir, un problema en el que observemos varias caracteristicas de un elemento y decidamos a que clase pertenece dicho elemento. \n",
        "\n",
        "Por ejemplo: Un algoritmo de clasificación de imagenes que intente saber si algo es un perro, un gato o un ratón. Si introdujeramos características y al final diera un vector $(0.2, 0.38, 0.42)$, el algoritmo lo clasificaría como un ratón (lo marcaría como perteneciente a la tercer clase)\n",
        "\n",
        "Cuando las predicciones son consistentes con las etiquetas verdaderas, decimos que son correctas. La presición de la clasificación son la cantidad de predicciones correctas entre todas las predicciones. Sin embargo, es muy difícil optimizar esto directamente (no es una función diferenciable), pero será muy importante para ver el rendimiento de nuestro modelo."
      ],
      "metadata": {
        "id": "kST4BHoWMH4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo que hacemos para ver la precisión es encontrar el valor mas grande en el vector de probabilidades, convertirlo en entero (porque muy probablemente sea un valor entr 0 y 1) y compararlo con las etiquetas verdaderas de nuestros datos."
      ],
      "metadata": {
        "id": "AbMsjaRTMHNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARGMAX regresa la posicion del elemento con mayor valor\n",
        "np.array([[0.2,0.38, 0.42]]).argmax(axis=1)"
      ],
      "metadata": {
        "id": "ULYR_9RjPljp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Calcula la cantidad de predicciones correctas.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.astype(y.dtype) == y\n",
        "    return float(cmp.astype(y.dtype).sum())/ len(y)"
      ],
      "metadata": {
        "id": "6WRt_0Ics488"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(y_hat, y) "
      ],
      "metadata": {
        "id": "ZMF7OSNCtGVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Veamos como sería el accuracy para nuestro modelo con pesos al azar"
      ],
      "metadata": {
        "id": "vx7hIiLYYy6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_prob"
      ],
      "metadata": {
        "id": "1Bx9EcuoQT10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(net(X_prob, W, B),Y_prob) "
      ],
      "metadata": {
        "id": "Jxzc1dXTYoaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.normal(0, 0.1, (num_inputs, num_outputs)) # Jugaremos con este parametro\n",
        "B = np.zeros(num_outputs) # Jugaremos con este parametro"
      ],
      "metadata": {
        "id": "BWjrsx5tZEFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Las probabilidades que predecimos para cada fila de nuestros datos\n",
        "net(X_prob, W, B)"
      ],
      "metadata": {
        "id": "1p1t9fb0YNxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La etiqueta verdadera de nuestros datos\n",
        "Y_prob"
      ],
      "metadata": {
        "id": "WlBh3aN-Y4GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SoftMax Estable\n"
      ],
      "metadata": {
        "id": "HMbRIr47vHUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El resultado final es lo que alimentará a la cross-entropy, es decir etiqueta \n",
        "# de que pertenece a la clase 1 por la probabilidad de pertenecer a la clase 1\n",
        "y = np.array([0, 2])\n",
        "y_hat = np.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y_hat[[0, 1], y]"
      ],
      "metadata": {
        "id": "kkI2X7gYvGWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función softmax (Repetida de arriba solo para ejemplificar)\n",
        "def softmax(X):\n",
        "    X_exp = np.exp(X)\n",
        "    partition = X_exp.sum(1, keepdims=True)\n",
        "    return X_exp / partition"
      ],
      "metadata": {
        "id": "Ki01nLOMu8af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Estable\n",
        "def stable_softmax(X):\n",
        "    X_exp = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    partition = X_exp.sum(1, keepdims=True)\n",
        "    return X_exp / partition"
      ],
      "metadata": {
        "id": "fWGnq6Fvzlkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net_s(X,W,b):\n",
        "    return stable_softmax(np.dot(X.reshape((-1, W.shape[0])), W) + b)"
      ],
      "metadata": {
        "id": "IAjDVD6xvRRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demostración de SoftMax estable"
      ],
      "metadata": {
        "id": "tP8jPUi-2fhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prueba = np.array([[99999, 99995, 99993],[9, 5, 3]])"
      ],
      "metadata": {
        "id": "A0ztJCHDz2iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(prueba)"
      ],
      "metadata": {
        "id": "7NAUCE4bzq9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stable_softmax(prueba)"
      ],
      "metadata": {
        "id": "w3Fxqeucz4aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si está repetida, es solo para recordar su estructura\n",
        "def cross_entropy(y_hat, y):\n",
        "    return np.sum(- np.log(y_hat[range(len(y_hat)), y])) / y.shape[0]"
      ],
      "metadata": {
        "id": "1QhptRqMu8ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy(y_hat, y)"
      ],
      "metadata": {
        "id": "iOSGd-A2vRsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Como seria el gradiente de mi función compuesta?\n",
        "\n",
        "$$ \\require{cancel} \n",
        "\\begin{align} \n",
        "\\frac{dL}{dz_i} &= \\frac{d}{dz_i} \\bigg[  – \\sum_{k=1}^c y_k log (a_k) \\bigg] \\\\ \n",
        "&= – \\sum_{k=1}^c y_k \\frac{d \\big( log (a_k) \\big)}{dz_i} \\\\ \n",
        "&= – \\sum_{k=1}^c y_k \\frac{d \\big( log (a_k) \\big)}{da_k} . \\frac{da_k}{dz_i} \\\\ \n",
        "&= – \\sum_{k=1}^c\\frac{y_k}{a_k} . \\frac{da_k}{dz_i} \\\\ \n",
        "&= – \\bigg[ \\frac{y_i}{a_i} . \\frac{da_i}{dz_i}  + \\sum_{k=1, k \\not=i}^c \\frac{y_k}{a_k}  \\frac{da_k}{dz_i} \\bigg] \\\\ \n",
        "&= – \\frac{y_i}{\\cancel{a_i}} . \\cancel{a_i}(1-a_i) \\text{ } – \\sum_{k=1, k \\not=i}^c \\frac{y_k}{\\cancel{a_k}} . (\\cancel{a_k}a_i) \\\\ \n",
        "&= – y_i +y_ia_i + \\sum_{k=1, k \\not=i}^c y_ka_i \\\\ \n",
        "&= a_i \\big( y_i + \\sum_{k=1, k \\not=i}^c y_k \\big) – y_i \\\\ \n",
        "&= a_i + \\sum_{k=1}^c y_k -y_i \\\\ \n",
        "&= a_i . 1 – y_i \\text{ , since } \\sum_{k=1}^c y_k =1 \\\\ \n",
        "&= a_i – y_i \n",
        "\\end{align} $$"
      ],
      "metadata": {
        "id": "tnvttqxF-HpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_softmax_cross_entropy_for_w(X, y_true, w, b):\n",
        "    netaux = net_s(X,w,b)\n",
        "\n",
        "    ones_true_class = np.zeros_like(netaux) # Matriz de ceros que tiene la forma de las probabilidades (logits.shape), si alimento todos los datos es 50000X2\n",
        "\n",
        "    ones_true_class[np.arange(len(netaux)),y_true] = 1 # Genera 1 o cero si esta en la clasificacion correcta\n",
        " \n",
        "    softmax_ = netaux # softmax aplicado a mis probabilidades\n",
        "\n",
        "    aux1 = softmax_ - ones_true_class\n",
        "\n",
        "    return X.T @ aux1/X.shape[0]"
      ],
      "metadata": {
        "id": "bdaasRLfu3cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_softmax_cross_entropy_for_b(X, y_true, w, b):\n",
        "    netaux = net_s(X,w,b)\n",
        "\n",
        "    ones_true_class = np.zeros_like(netaux) # Matriz de ceros que tiene la forma de las probabilidades (logits.shape), si alimento todos los datos es 50000X2\n",
        "\n",
        "    ones_true_class[np.arange(len(netaux)),y_true] = 1 # Genera 1 o cero si esta en la clasificacion correcta\n",
        " \n",
        "    softmax_ = netaux # softmax aplicado a mis probabilidades\n",
        "\n",
        "    aux1 = softmax_ - ones_true_class\n",
        "\n",
        "    return aux1.sum(axis=0, keepdims=True)/X.shape[0]"
      ],
      "metadata": {
        "id": "vPmuEewJvWyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos implementar Gradient Descent"
      ],
      "metadata": {
        "id": "Iqm6FZHOv5kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w, b, net, learning_rate=0.1, num_epochs=10):\n",
        "\tm = x.shape[0]\n",
        "\tJ_all = []\n",
        "\t\n",
        "\tfor _ in range(num_epochs):\n",
        "\t\ty_probs = net_s(x, w, b)\n",
        "\t\ty_hat = y_probs.argmax(axis=1)\n",
        "\t\t\n",
        "\t\tgradient_w = grad_softmax_cross_entropy_for_w(x,y,w,b)\n",
        "\t\tgradient_b = grad_softmax_cross_entropy_for_b(x,y,w,b)\n",
        "   \n",
        "\t #Actualización de los parámetros\n",
        "\t\tw = w - (learning_rate)*gradient_w\n",
        "\t\tb = b - (learning_rate)*gradient_b\n",
        "\t\t\n",
        "\t\tJ_all.append(cross_entropy(y_probs, y))\n",
        "\n",
        "\treturn J_all, w, b"
      ],
      "metadata": {
        "id": "xaj_uBU4LxfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.normal(0, 0.1, (num_inputs, num_outputs)) # Jugaremos con este parametro\n",
        "B = np.zeros(num_outputs) # Jugaremos con este parametro"
      ],
      "metadata": {
        "id": "VyLdbdMdNmAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_epochs = 1000"
      ],
      "metadata": {
        "id": "fdtOu8Gv4phz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J_all, W, B = gradient_descent(X_prob, Y_prob, W, B, net, learning_rate, num_epochs)\n",
        "Y_hat = net(X_prob, W, B)\n",
        "J = cross_entropy(Y_hat, Y_prob)\n",
        "print(\"Cost: \", J)\n",
        "print(\"Parameters: \", W)\n",
        "print(\"Intercepto: \", B)"
      ],
      "metadata": {
        "id": "TWTzZZQcBSEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(J_all)"
      ],
      "metadata": {
        "id": "idROj5bqPLSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perdida_por_epoca = np.array(J_all).reshape(-1,1)\n",
        "plt.plot(perdida_por_epoca)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvlLiqkyQOXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Las probabilidades que predecimos para cada fila de nuestros datos\n",
        "net(X_prob, W, B)"
      ],
      "metadata": {
        "id": "FhTSxna-NKqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La etiqueta verdadera de nuestros datos\n",
        "Y_prob"
      ],
      "metadata": {
        "id": "YYmxh4cRNKqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(net(X_prob, W, B),Y_prob) "
      ],
      "metadata": {
        "id": "ipkiKHVyNZ8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD: Stochastic Gradient Descent\n",
        "\n",
        "En este caso, el algoritmo entrenara de modo un poco distinto, tomando muestras al azar de mis datos, dicha muestra será del tamaño que yo la pida."
      ],
      "metadata": {
        "id": "ABi5C3Kg4kkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200"
      ],
      "metadata": {
        "id": "t2x4Fyr16myj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_rows = X_prob.shape[0]  # En nuestro caso, son 50,000\n",
        "random_indices = np.random.choice(number_of_rows, size=batch_size, replace=False)\n",
        "X_prima = X_prob[random_indices, :].copy()"
      ],
      "metadata": {
        "id": "a1pc7Mns468G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prima"
      ],
      "metadata": {
        "id": "mS-sbE2r8Oul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prima = Y_prob[random_indices].copy()"
      ],
      "metadata": {
        "id": "9hmnUV6W5aBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prima"
      ],
      "metadata": {
        "id": "uS3OZYk_8Sbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(x, y, w, b, net, batch_size, learning_rate=0.1, num_epochs=10):\n",
        "  m = x.shape[0]\n",
        "  J_all = []\n",
        "\t\n",
        "  for _ in range(num_epochs):\n",
        "    random_indices = np.random.choice(m, size=batch_size, replace=False)\n",
        "    x_batch = x[random_indices, :].copy()\n",
        "    y_batch = y[random_indices].copy()\n",
        "\n",
        "    y_probs = net_s(x_batch, w, b)\n",
        "    #y_hat = y_probs.argmax(axis=1)\n",
        "    \n",
        "    gradient_w = grad_softmax_cross_entropy_for_w(x_batch,y_batch,w,b)\n",
        "    gradient_b = grad_softmax_cross_entropy_for_b(x_batch,y_batch,w,b)\n",
        "\n",
        "    #Actualizamos los paramentros\n",
        "    w = w - (learning_rate)*gradient_w\n",
        "    b = b - (learning_rate)*gradient_b\n",
        "    \n",
        "    J_all.append(cross_entropy(y_probs, y_batch))\n",
        "\n",
        "  return J_all, w, b "
      ],
      "metadata": {
        "id": "wkEVafDv6zSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.normal(0, 0.1, (num_inputs, num_outputs)) # Jugaremos con este parametro\n",
        "B = np.zeros(num_outputs) # Jugaremos con este parametro"
      ],
      "metadata": {
        "id": "tCHpmpEL76dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_epochs = 1000"
      ],
      "metadata": {
        "id": "577ys26F8ECc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J_all, W, B = batch_gradient_descent(X_prob, Y_prob, W, B, net, batch_size, learning_rate, num_epochs)\n",
        "Y_hat = net(X_prob, W, B)\n",
        "J = cross_entropy(Y_hat, Y_prob)\n",
        "print(\"Cost: \", J)\n",
        "print(\"Parameters: \", W)\n",
        "print(\"Intercepto: \", B)"
      ],
      "metadata": {
        "id": "mI0orbzN8ECc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(J_all)"
      ],
      "metadata": {
        "id": "FM4kNcrD8ECd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perdida_por_epoca = np.array(J_all).reshape(-1,1)\n",
        "plt.plot(perdida_por_epoca)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B7WbR8pv8ECd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Las probabilidades que predecimos para cada fila de nuestros datos\n",
        "net(X_prob, W, B)"
      ],
      "metadata": {
        "id": "kLhi1JnF8ECd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La etiqueta verdadera de nuestros datos\n",
        "Y_prob"
      ],
      "metadata": {
        "id": "nQu21Iv_8ECd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(net(X_prob, W, B),Y_prob) "
      ],
      "metadata": {
        "id": "8jsZGmHu8ECd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}