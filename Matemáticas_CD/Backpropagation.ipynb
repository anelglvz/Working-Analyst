{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anelglvz/Working-Analyst/blob/main/Matem%C3%A1ticas_CD/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb2BomaxRVnf"
      },
      "source": [
        "# Red Neuronal\n",
        "\n",
        "En esta sesión instanciaremos un objeto de clase red neuronal (que nosotros crearemos). Veremos como funciona el feedforward (no tan complicado dada la clase pasada) y despues veremos backpropagation (un poco menos fácil, es una generalización de la red que se entreno la semana 2, la que constaba de una sola capa).\n",
        "\n",
        "Se utilizará la funcion de perdida de Error Cuadrático Medio, así como funciones de activación distintas (podrá ser ReLu, Sigmoide y Lineal)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSq0n_7URVnh"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUgPMOfFRVni"
      },
      "outputs": [],
      "source": [
        "# Proceso que se utiliza para convertir características categóricas en características binarias\n",
        "def one_hot_encode(y, num_classes):\n",
        "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
        "    idx = [np.arange(y.shape[0]), y]\n",
        "    y_one_hot[tuple(idx)] = 1\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplificando \"One Hot Encoding\" (ayuda en problemas de clasificación)"
      ],
      "metadata": {
        "id": "0SXde2RXQNRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea nuevas columnas en la que me dice si el elemento pertenece a la categoría 0, a la 1 o a la 2\n",
        "encoded = one_hot_encode(np.array([1, 0, 2, 1]), 3)\n",
        "encoded"
      ],
      "metadata": {
        "id": "UniXsHZNQRW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función de perdida"
      ],
      "metadata": {
        "id": "XPFFGSC2QRm0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fua1LbPRVnj"
      },
      "outputs": [],
      "source": [
        "# Nuestra función de perdida (Error cuadrátio medio)\n",
        "def mse(y_hat, y):\n",
        "    return np.mean((y - y_hat)**2)\n",
        "\n",
        "# Dado su nombre y forma, ¿quien es esta función?\n",
        "def d_mse(y_hat, y):\n",
        "    return (1 / y_hat.shape[0]) * (1/y_hat.shape[-1]) * -2 * np.sum(y - y_hat, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uno = np.array([1,2,3,4,5])\n",
        "dos = np.array([1,2,3,4,7])\n",
        "mse(uno, dos)"
      ],
      "metadata": {
        "id": "nhvmaN95Qz2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_to_encoded = np.array([[1., 0., 0.],\n",
        "                              [1., 0., 0.],\n",
        "                              [0., 0., 1.],\n",
        "                              [0., 1., 0.]])"
      ],
      "metadata": {
        "id": "Y9s9p19pI-fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse(encoded, compare_to_encoded)"
      ],
      "metadata": {
        "id": "sBa3JiOVJH-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones de activación"
      ],
      "metadata": {
        "id": "1-av6dgAw6bO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2imCJxKRVnj"
      },
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def d_linear(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.where(x > 0, x, 0)\n",
        "\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def d_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Que funciones de activación no están aquí?"
      ],
      "metadata": {
        "id": "cMks0LqXw9XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación de la clase Red Neuronal"
      ],
      "metadata": {
        "id": "Ne1X7gGnxL-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atributos que tendrá nuestro objeto:\n",
        "\n",
        "- Pesos\n",
        "- Sesgos\n",
        "- Tape *\n",
        "- Función de activación\n",
        "- Derivadas de las activaciones\n",
        "\n",
        "Métodos que tendrá:\n",
        "\n",
        "- __ init __\n",
        "- forward\n",
        "- backward\n",
        "- predict"
      ],
      "metadata": {
        "id": "owMwYTilWxRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 3\n",
        "hidden_sizes = [3,4]\n",
        "output_size = 1"
      ],
      "metadata": {
        "id": "s-IiMoAUcg0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRTCSyvRVnj"
      },
      "outputs": [],
      "source": [
        "# Creación de nuestra clase red, con metodos \"forward\" para avanzar sobre nuestra red, \"backward\" para optimizar parámetros, \"predict\" para lo obvio\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            n_in = layer_sizes[i]\n",
        "            n_out = layer_sizes[i + 1]\n",
        "            self.weights.append(np.random.randn(n_in, n_out))\n",
        "            self.biases.append(np.random.randn(1, n_out))\n",
        "\n",
        "        self.tape = [None for _ in range(len(self.weights) + 1)]\n",
        "        self.activations = [linear] + [relu for _ in range(len(self.weights) - 1)] + [sigmoid]\n",
        "        self.d_activations = [d_linear] + [d_relu for _ in range(len(self.weights) - 1)] + [d_sigmoid]\n",
        "\n",
        "    def forward(self, x, grad=False):\n",
        "        if grad and self.tape[0] is not None:\n",
        "            raise ValueError(\"Cannot call forward with grad without calling backwards\")\n",
        "\n",
        "        if grad:\n",
        "            self.tape[0] = x\n",
        "        \n",
        "        for i in range(len(self.weights)):\n",
        "            x_hat = x @ self.weights[i] + self.biases[i]\n",
        "            x = self.activations[i + 1](x_hat)\n",
        "            if grad:\n",
        "                self.tape[i + 1] = x_hat\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backward(self, d_loss):\n",
        "        assert d_loss.shape == self.weights[-1].shape[-1:]\n",
        "\n",
        "        weights_grad = [None for w in self.weights]\n",
        "        biases_grad = [None for b in self.biases]\n",
        "\n",
        "        d_activation = self.d_activations[-1]\n",
        "        error = d_loss * d_activation(self.tape[-1]) # (n_out) * (n_out)\n",
        "        error = error.reshape(1, -1)\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            # error = (1, n_out)\n",
        "            # tape[i] = (n_in)\n",
        "            # weights[i] = (n_in, n_out)\n",
        "            x = self.tape[i]\n",
        "            activation = self.activations[i]\n",
        "            d_activation = self.d_activations[i]\n",
        "\n",
        "            weights_grad[i] = error * activation(x.reshape(-1, 1)) # (1, n_out) * (n_in, 1) -> (n_in, n_out) * (n_in, n_out) por broadcast -> (n_in, n_out)\n",
        "            biases_grad[i] = error * 1 # derivada de los sesgos es un vector de unos, lo representamos explicitamente como 1 (uso de broadcasting)\n",
        "            \n",
        "            error = error @ self.weights[i].T  # (1, n_out) @ (n_out, n_in) -> (1, n_in)\n",
        "            error = error * d_activation(x).reshape(1, -1) # (1, n_in) * (1, n_in)\n",
        "\n",
        "\n",
        "        self.tape = [None for _ in range(len(self.weights) + 1)]\n",
        "\n",
        "        return weights_grad, biases_grad\n",
        "\n",
        "    def predict(self, x):\n",
        "        y = self.forward(x)\n",
        "        preds = np.argmax(y, axis=1) # Funciona para algoritmos de clasificación\n",
        "        return preds # la salida sería \"y\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh-QfjEHRVnk"
      },
      "source": [
        "# Revisando que la PreAlimentación (Feed Forward) funcione correctamente"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizaremos \"assert\", el cual regresa un aviso si la condición no se cumple\n",
        "assert 1 == 2"
      ],
      "metadata": {
        "id": "53KBmBnaUYCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfOjinrERVnl"
      },
      "outputs": [],
      "source": [
        "# Revisión\n",
        "model = NeuralNetwork(2, [4], 1)\n",
        "\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "targets = np.array([0, 1, 2, 1]).reshape(-1,1)\n",
        "\n",
        "loss = np.mean((targets - sigmoid(relu(inputs @ model.weights[0] + model.biases[0]) @ model.weights[1] + model.biases[1]))**2)   ## CASO 1 sola intermedia\n",
        "assert np.allclose(loss, mse(model.forward(inputs), targets))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse(model.forward(inputs), targets)"
      ],
      "metadata": {
        "id": "3BhqVSKxa8vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio: \n",
        "\n",
        "- ¿Que pasa en el código anterior si doy como capas intermedias la lista [3,4]? ¿Como evitar ese assertion error?"
      ],
      "metadata": {
        "id": "IqBTnc26WiB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUCION EJERCICIO\n",
        "model = NeuralNetwork(2, [3,4], 1)\n",
        "\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "targets = np.array([0, 1, 2, 1]).reshape(-1,1)\n",
        "\n",
        "loss = np.mean((targets - sigmoid(relu(inputs @ model.weights[0] + model.biases[0]) @ model.weights[1] + model.biases[1]))**2)   ## CASO 1 sola intermedia\n",
        "#loss = np.mean((targets - sigmoid( relu(  relu(inputs @ model.weights[0] + model.biases[0]) @ model.weights[1] + model.biases[1]) @ model.weights[2] + model.biases[2]  )  )**2)\n",
        "\n",
        "assert np.allclose(loss, mse(model.forward(inputs), targets))"
      ],
      "metadata": {
        "id": "kb7Up8dMWrQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veamos los atributos de nuestro objeto \"model\""
      ],
      "metadata": {
        "id": "NOJyd3-gXN4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights"
      ],
      "metadata": {
        "id": "ut4P3pYvXSA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.biases"
      ],
      "metadata": {
        "id": "AHEVBn2AXVLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs @ model.weights[0] + model.biases[0]"
      ],
      "metadata": {
        "id": "JfWzRn2juasc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.activations"
      ],
      "metadata": {
        "id": "CW-vqbePXWNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.d_activations"
      ],
      "metadata": {
        "id": "QMYMxF0k7GU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgE1MIH5RVnm"
      },
      "source": [
        "# Analytical Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us8j0spsRVnm"
      },
      "outputs": [],
      "source": [
        "def compute_analytical_gradient(model, X, Y, batch_size, num_classes):\n",
        "    analytical_weights_grad = [np.zeros(w.shape) for w in model.weights]\n",
        "    analytical_biases_grad = [np.zeros(b.shape) for b in model.biases]\n",
        "\n",
        "    batches = list(zip(X, Y))\n",
        "    for i in range(0, len(batches), batch_size):\n",
        "        batch = batches[i : i + batch_size]\n",
        "        x, y = zip(*batch)\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "        y_one_hot = one_hot_encode(y, num_classes) # y   # cuando no sea problema de clasificación\n",
        "        y_hat = model.forward(x, grad=True)\n",
        "\n",
        "        weights_grad, biases_grad = model.backward(d_mse(y_hat, y_one_hot))\n",
        "        for l in range(len(model.weights)):\n",
        "            analytical_weights_grad[l] += weights_grad[l]\n",
        "            analytical_biases_grad[l] += biases_grad[l]\n",
        "\n",
        "    for l in range(len(model.weights)):\n",
        "        analytical_weights_grad[l] /= len(X)\n",
        "        analytical_biases_grad[l] /= len(X)\n",
        "\n",
        "    return analytical_weights_grad, analytical_biases_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Puede ser que la derivada analítica de mis funciones no exista?"
      ],
      "metadata": {
        "id": "z6BhaZmryAGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yoHT5ODRVnm"
      },
      "outputs": [],
      "source": [
        "# No ES NECESARIA\n",
        "def compute_numerical_gradient(model, X, Y, batch_size, num_classes, delta=1e-8):\n",
        "    batches = list(zip(X, Y))\n",
        "\n",
        "    def compute_loss():\n",
        "        loss = 0\n",
        "        for i in range(0, len(batches), batch_size):\n",
        "            batch = batches[i : i + batch_size]\n",
        "            x, y = zip(*batch)\n",
        "            x = np.array(x)\n",
        "            y = np.array(y)\n",
        "            y_one_hot = one_hot_encode(y, num_classes)\n",
        "            y_hat = model.forward(x)\n",
        "            loss += mse(y_hat, y_one_hot) * len(x)\n",
        "        return loss / len(X)\n",
        "\n",
        "    base_loss = compute_loss()\n",
        "\n",
        "    numerical_weights_grad = [np.zeros(w.shape) for w in model.weights]\n",
        "    numerical_biases_grad = [np.zeros(b.shape) for b in model.biases]\n",
        "    for l in range(len(model.weights)):\n",
        "        for ix, iy in np.ndindex(model.weights[l].shape):\n",
        "            old_w = model.weights[l][ix][iy]\n",
        "            new_w = old_w + delta\n",
        "            model.weights[l][ix][iy] = new_w\n",
        "\n",
        "            new_loss = compute_loss()\n",
        "            dL = base_loss - new_loss\n",
        "            dw = old_w - new_w\n",
        "            numerical_weights_grad[l][ix][iy] = dL / dw\n",
        "\n",
        "            model.weights[l][ix][iy] = old_w\n",
        "\n",
        "        for ii in range(len(model.biases[l][0])):\n",
        "            old_b = model.biases[l][0][ii]\n",
        "            new_b = old_b + delta\n",
        "            model.biases[l][0][ii] = new_b\n",
        "\n",
        "            new_loss = compute_loss()\n",
        "            dL = base_loss - new_loss\n",
        "            db = old_b - new_b\n",
        "            numerical_biases_grad[l][0][ii] = dL / db\n",
        "\n",
        "            model.biases[l][0][ii] = old_b\n",
        "\n",
        "    return numerical_weights_grad, numerical_biases_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bz38E26RVnm"
      },
      "source": [
        "# Revision de algoritmo de BackPropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwZpRQFnRVnn"
      },
      "outputs": [],
      "source": [
        "input_size = 2\n",
        "hidden_sizes = [4]\n",
        "output_size = 1\n",
        "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "\n",
        "n_examples = 1000\n",
        "n_classes = 1\n",
        "x = np.random.randn(n_examples, input_size)\n",
        "y = np.random.randint(0, n_classes, size=(n_examples, n_classes))\n",
        "\n",
        "numerical_weights_grad, numerical_biases_grad = compute_numerical_gradient(\n",
        "    model, x, y, batch_size=1, num_classes=n_classes\n",
        ")\n",
        "analytical_weights_grad, analytical_biases_grad = compute_analytical_gradient(\n",
        "    model, x, y, batch_size=1, num_classes=n_classes\n",
        ")\n",
        "\n",
        "# check numerical and analytical gradients have same shape\n",
        "assert len(numerical_weights_grad) == len(analytical_weights_grad) == len(analytical_biases_grad) == len(numerical_biases_grad)\n",
        "for i in range(len(numerical_weights_grad)):\n",
        "    assert numerical_weights_grad[i].shape == analytical_weights_grad[i].shape\n",
        "    assert numerical_biases_grad[i].shape == analytical_biases_grad[i].shape\n",
        "\n",
        "\n",
        "for l in range(len(model.weights)):\n",
        "    with np.printoptions(precision=4, suppress=True):\n",
        "        print(f\"---- layer {l} ----\")\n",
        "        print(\"numerical weights gradient:\\n\", numerical_weights_grad[l])\n",
        "        print(\"analytical weights gradient:\\n\", analytical_weights_grad[l])\n",
        "        print()\n",
        "        print(\"numerical biases gradient:\\n\", numerical_biases_grad[l])\n",
        "        print(\"analytical biases gradient:\\n\", analytical_biases_grad[l])\n",
        "        print()\n",
        "\n",
        "        if not np.allclose(analytical_weights_grad[l], numerical_weights_grad[l], atol=1e-6):\n",
        "            print(f\"---- layer {l} ----\")\n",
        "            print(\"numerical weights gradient:\\n\", numerical_weights_grad[l])\n",
        "            print(\"analytical weights gradient:\\n\", analytical_weights_grad[l])\n",
        "            raise ValueError(\"incorrect backprop\")\n",
        "\n",
        "        if not np.allclose(analytical_biases_grad[l], numerical_biases_grad[l], atol=1e-6):\n",
        "            print(f\"---- layer {l} ----\")\n",
        "            print(\"numerical biases gradient:\\n\", numerical_biases_grad[l])\n",
        "            print(\"analytical biases gradient:\\n\", analytical_biases_grad[l])\n",
        "            raise ValueError(\"incorrect backprop\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g02ngFUWRVnn"
      },
      "outputs": [],
      "source": [
        "# with np.printoptions(precision=4, suppress=True):\n",
        "#     print(analytical_biases_grad[1] / numerical_biases_grad[1])\n",
        "#     print()\n",
        "#     print(analytical_weights_grad[1] / numerical_weights_grad[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17WTRSFjRVnn"
      },
      "source": [
        "# Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF-0inTfRVno"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # Ya se importó al inicio\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5_hB0t3RVno"
      },
      "outputs": [],
      "source": [
        "def generate_area_map(features, points_per_int=10, alpha=0.2):\n",
        "    xstart = int((features[:, 0].min() - 1) * points_per_int)\n",
        "    xrang = int((features[:, 0].max() + 1) * points_per_int - xstart)\n",
        "\n",
        "    ystart = int((features[:, 1].min() - 1) * points_per_int)\n",
        "    yrang = int((features[:, 1].max() + 1) * points_per_int - ystart)\n",
        "\n",
        "    area_map_set = np.array(\n",
        "        [[x + xstart, y + ystart] for x in range(xrang) for y in range(yrang)]\n",
        "    )\n",
        "    area_map_set = area_map_set / points_per_int\n",
        "\n",
        "    return area_map_set\n",
        "\n",
        "\n",
        "def area_map_plot(network, area_map_set, features, targets, path=\"\", alpha=0.1):\n",
        "    pred = network.predict(area_map_set)\n",
        "\n",
        "    plt.scatter(features[:, 0], features[:, 1], c=targets, cmap=\"jet\")\n",
        "    plt.scatter(area_map_set[:, 0], area_map_set[:, 1], c=pred, alpha=alpha, cmap=\"jet\")\n",
        "\n",
        "    if path == \"\":\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQOPqqs7RVno"
      },
      "outputs": [],
      "source": [
        "X_all, y_all = make_blobs(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    centers=3,\n",
        "    center_box=(-1, 1),\n",
        "    cluster_std=0.25,\n",
        "    random_state=12345,\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all, y_all, test_size=0.25, stratify=y_all\n",
        ")\n",
        "print(\"X_train shape =\", X_train.shape)\n",
        "print(\"X_test shape =\", X_test.shape)\n",
        "print(\"y_train shape =\", y_train.shape)\n",
        "print(\"y_test shape =\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "DT6ColER-6e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "RGQNHIhKmCXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LX7gMtE6RVnp"
      },
      "outputs": [],
      "source": [
        "print(\"train set\")\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"jet\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER-zCB6uRVnp"
      },
      "outputs": [],
      "source": [
        "print(\"test set\")\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"jet\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo3UWbMWRVnp"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrSgY1C9RVnq"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(model, X, Y, lr, batch_size, num_classes):\n",
        "    analytical_weights_grad, analytical_biases_grad = compute_analytical_gradient(model, X, Y, batch_size, num_classes)\n",
        "\n",
        "    running_loss = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        # one hot encode labels\n",
        "        x = x.reshape(1, -1)\n",
        "        y = y.reshape(1)\n",
        "        y_one_hot = one_hot_encode(y, num_classes)\n",
        "\n",
        "        # feed forward\n",
        "        y_hat = model.forward(x, grad=True)\n",
        "\n",
        "        # compute loss and its derivative\n",
        "        loss = mse(y_hat, y_one_hot)\n",
        "        d_error = d_mse(y_hat, y_one_hot)\n",
        "        running_loss += loss\n",
        "\n",
        "        # backpropagate the error (update gradients)\n",
        "        model.backward(d_error)\n",
        "\n",
        "        # gradient descent step\n",
        "        for i in range(len(model.weights)):\n",
        "            # update weights\n",
        "            model.weights[i] += lr * -analytical_weights_grad[i]\n",
        "            model.biases[i] += lr * -analytical_biases_grad[i]\n",
        "\n",
        "            # reset gradients\n",
        "            # model.weights_grad[i].fill(0)\n",
        "            # model.biases_grad[i].fill(0)\n",
        "\n",
        "    return running_loss / len(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NO EJECUTAR\n",
        "def numerical_gradient_descent(model, X, Y, lr, batch_size, num_classes):\n",
        "    numerical_weights_grad, numerical_biases_grad = compute_numerical_gradient(\n",
        "        model, X, Y, batch_size=batch_size, num_classes=num_classes\n",
        "    )\n",
        "    for l in range(len(model.weights)):\n",
        "        model.weights[l] += lr * -numerical_weights_grad[l]\n",
        "        model.biases[l] += lr * -numerical_biases_grad[l]\n",
        "\n",
        "    loss = 0\n",
        "    batches = list(zip(X, Y))\n",
        "    for i in range(0, len(batches), batch_size):\n",
        "        batch = batches[i : i + batch_size]\n",
        "        x, y = zip(*batch)\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "        y_one_hot = one_hot_encode(y, num_classes)\n",
        "        y_hat = model.forward(x)\n",
        "        loss += mse(y_hat, y_one_hot) * len(x)\n",
        "    loss /= len(X)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "DQrSL9twAmsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFD6SIE-RVnq"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork(2, [5], 3)\n",
        "\n",
        "area_map_set = generate_area_map(X_test, points_per_int = 7)\n",
        "area_map_plot(model, area_map_set, X_test, y_test, alpha = 0.15)\n",
        "\n",
        "print(\"Accuracy on Train Set:\", np.mean(model.predict(X_train) == y_train))\n",
        "print(\"Accuracy on Test Set:\", np.mean(model.predict(X_test) == y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights[0].shape"
      ],
      "metadata": {
        "id": "3PxNcHEq_XGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.biases[0].shape"
      ],
      "metadata": {
        "id": "UWd2nVFG_usU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.weights[1].shape"
      ],
      "metadata": {
        "id": "C1O8P0Sn_ptg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.biases[1].shape"
      ],
      "metadata": {
        "id": "85Jx3vLq_3VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nwXzL5BRVnr"
      },
      "outputs": [],
      "source": [
        "loss_history = []\n",
        "acc_history = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = stochastic_gradient_descent(model, X_train, y_train, lr=0.01, batch_size=1 ,num_classes=3)\n",
        "    acc = np.mean(model.predict(X_test) == y_test)\n",
        "    loss_history.append(loss)\n",
        "    acc_history.append(acc)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"epoch: {epoch:<6} loss: {loss:.6f}     accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjBX9dmzRVns"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGzafIVIRVnt"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_history)\n",
        "plt.title(\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26uGtrg-RVnt"
      },
      "outputs": [],
      "source": [
        "plt.plot(acc_history)\n",
        "plt.title(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OjY9K69RVnu"
      },
      "outputs": [],
      "source": [
        "area_map_set = generate_area_map(X_test, points_per_int = 7)\n",
        "area_map_plot(model, area_map_set, X_test, y_test, alpha = 0.15)\n",
        "\n",
        "print(\"Accuracy on Train Set:\", np.mean(model.predict(X_train) == y_train))\n",
        "print(\"Accuracy on Test Set:\", np.mean(model.predict(X_test) == y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "xAFS4x9VCCpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test == model.predict(X_test)"
      ],
      "metadata": {
        "id": "VPZUo-4SpMhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ligas de interes:\n",
        "\n",
        "- https://wordpress.cs.vt.edu/optml/2018/04/28/backpropagation-is-not-just-the-chain-rule/#:~:text=Basically%2C%20machine%20learning%20problem%20is,%CE%B8)%2C%20to%20predict%20y"
      ],
      "metadata": {
        "id": "o91igpNjbduW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}